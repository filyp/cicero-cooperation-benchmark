{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from secret import API_KEY\n",
    "from game_data_utils import get_all_phase_dialogues, build_dialogue_text\n",
    "from constants import *\n",
    "from prompt_templates import *\n",
    "from api_utils import get_rating, completion_cached\n",
    "\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "# https://beta.openai.com/docs/api-reference/completions/create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the prompts\n",
    "for phase_dialogue, cicero_power, human_power in get_all_phase_dialogues():\n",
    "    print(f\"Cicero is: {cicero_power}\\n\")\n",
    "    text = build_dialogue_text(phase_dialogue)\n",
    "    print(text)\n",
    "    print(\"=====================================================\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer random dialogue\n",
    "dialogue, cicero_power, human_power = random.choice(list(get_all_phase_dialogues()))\n",
    "\n",
    "text = build_dialogue_text(dialogue)\n",
    "prompt = cooperation_prompt1.format(cicero_power=cicero_power, human_power=human_power)\n",
    "\n",
    "print(f\"( Cicero is: {cicero_power} )\\n\")\n",
    "print(text + prompt)\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    # model=\"text-curie-001\",\n",
    "    prompt=text + prompt,\n",
    "    max_tokens=300,\n",
    ")[\"choices\"][0][\"text\"]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating_for_dialogue(dialogue, cicero_power, human_power, prompt_template):\n",
    "    text = build_dialogue_text(dialogue)\n",
    "    prompt = prompt_template.format(cicero_power=cicero_power, human_power=human_power)\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        # model=\"text-davinci-003\",\n",
    "        model=\"text-curie-001\",\n",
    "        prompt=text + prompt,\n",
    "        max_tokens=300,\n",
    "        temperature=0,\n",
    "    )[\"choices\"][0][\"text\"]\n",
    "\n",
    "    rating = get_rating(response)\n",
    "\n",
    "    return dialogue, cicero_power, human_power, response, rating\n",
    "\n",
    "\n",
    "limit = 600\n",
    "dialogues_to_test = list(get_all_phase_dialogues())[:limit]\n",
    "\n",
    "# use ThreadPoolExecutor to parallelize the requests\n",
    "with ThreadPoolExecutor(max_workers=100) as executor:\n",
    "    dialogue_ratings = list(tqdm(executor.map(\n",
    "        lambda args: get_rating_for_dialogue(*args, cooperation_prompt1),\n",
    "        dialogues_to_test\n",
    "    ), total=len(dialogues_to_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count and print how many dialogs have None rating\n",
    "print(f\"Number of dialogs with None rating:  {sum(1 for info in dialogue_ratings if info[-1] is None)}\")\n",
    "print(f\"Number of dialogs with rating:       {sum(1 for info in dialogue_ratings if info[-1] is not None)}\")\n",
    "\n",
    "# get a histogram of ratings\n",
    "ratings = [info[-1] for info in dialogue_ratings if info[-1] is not None]\n",
    "plt.hist(ratings, bins=np.arange(0.5, 11.5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dialogs where rating low\n",
    "for info in dialogue_ratings:\n",
    "    rating = info[-1]\n",
    "    if rating is None:\n",
    "        continue\n",
    "    if rating < 5:\n",
    "        text = build_dialogue_text(info[0])\n",
    "        print(f\"( rating = {info[-1]} )\")\n",
    "        print(f\"( Cicero is: {info[1]} )\\n\")\n",
    "        print(text)\n",
    "        print(info[3])\n",
    "        print(\"\\n=====================================================\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze dialogs where rating is None\n",
    "for info in dialogue_ratings:\n",
    "    rating = info[-1]\n",
    "    if rating is None:\n",
    "        text = build_dialogue_text(info[0])\n",
    "        print(f\"( rating = {info[-1]} )\")\n",
    "        print(f\"( Cicero is: {info[1]} )\\n\")\n",
    "        print(text)\n",
    "        print(info[3])\n",
    "        print(\"\\n=====================================================\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
